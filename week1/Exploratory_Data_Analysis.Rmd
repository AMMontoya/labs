# Exploratory Data Analysis

Biases, systematic errors and unexpected variability are common in genomics data. Failure to discover these problems often leads to flawed analyses and false discoveries. As an example, consider that experiments sometimes fail and not all data processing pipelines are design to detect these and report an answer. Yet without removing data from failed experiments may result hard to notice downstream errors. In later modules we will cover many other examples. 

Graphing data is a powerful approach to detecting these problems. We refer to this as _exploratory data analyis_ (EDA). Many important methodological contributions to genomics data analysis were initiated as discovery made via EDA. We will show some useful exploratory plots for gene expression data measured with microarrays and NGS. We start with a general introduction to EDA using height data.
## Histograms

We can think of any given dataset as a list of numbers. Suppose you have measured the heights of all men in a population. Imagine you have need to describe these numbers to someone that has no idea what these heights are, say an alien that has never visited earth. 

```{r definingHeights, message=FALSE}
library(UsingR)
x=father.son$fheight
```
One approach is to simply list out all the numbers for the alien to see. Here are 100 randomly selected heights of 1,078.

```{r}
sample(x,20)
```

From scanning through these numbers we start getting a rough idea of what the entire list looks like but it certainly inefficient. We can quickly improve on this approach by creating bins, say by rounding each value to its nearest integer, and reporting the number of individuals in each bin. A plot of these heights is called a histogram
```{r histogram, fig.width=4, fig.height=4}
hist(x,breaks=seq(floor(min(x)),ceiling(max(x))))
```
Showing this plot to the alien is much more informative than showing the numbers. Note that with this simple plot we can approximate the number of individuals in any given interval. For example, there are about 70 individuals over six feet (72 inches) tall. 

## Normal approximation

If instead of the total numbers we report the proportions, then the histogram is a probability distribution. The probability distribution we see above approximates one that is very common in a nature: the bell curve or normal distribution or Gaussian distribution. When the histogram of a list of numbers approximates the normal distribution we can use a convenient mathematical formula to approximate the proportion of individuals in any given interval

$$
\mbox{Pr}(a < x < b) = \int_a^b \frac{1}{\sqrt{2\pi\sigma}} \exp{\left( \frac{x-\mu}{\sigma} \right)^2} \, dx
$$

Here $\mu$ and $\sigma$ are refereed to as the mean and standard deviation. If this approximation holds for our list then the population mean and variance of our list can be used in the formula above. To see this with an example remember that above we noted that 70 individuals or 6% of our population were taller than 6 feet. The normal approximation works well:
```{r}
1-pnorm(72,mean(x),sd(x)) 
```

A very useful characteristic of this approximation is that one only needs to know $\mu$ and $\sigma$ to describe the entire distribution. All we really have to tell our alien friend is that heights follow a normal distribution with average height 68'' and a standard deviation of 3''. From this we can compute the proportion of individuals in any interval. 

## QQ-plot

To corroborate that the normal distribution is in fact a good approximation we can use quantile-quantile plots. Quantiles are best understood by considering the special case of percentiles. The p-th percentile of a list of a distribution is defined as the number q that is bigger than p% of numbers. For example, the median 50-th percentile is the median. We can compute the percentiles for our list and for the normal distribution
```{r}
ps <- seq(0.01,0.99,0.01)
qs <- quantile(x,ps)
normalqs <- qnorm(ps,mean(x),sd(x))
plot(normalqs,qs)
abline(0,1) ##identity line
```
Note how close these values are. Also note that we can do same with less code
```{r}
qqnorm(y)
qqline(y)
```

## Box-plot
Data is not always normally distributed. Income 
In these cases the average and standard deviation are not necessarily informative. The properties described above are specific to the normal. For example, the normal distribution does not seem to be a good approximation for the direct compensation for 199 United States CEOs in the year 2000
```{r, fig.width=4, fig.height=4}
hist(exec.pay)
```
A practical summary is to compute 3 percentiles: 25-th, 50-th (the median) and the 75-th. A boxplots shows these 3 values along with a range calculated as median $\pm$ 1.5 75-th percentiles - 25th-percentile. Values outside this range are shown as points.

```{r,fig.width=4, fig.height=4}
boxplot(exec.pay,ylab="10,000s of dollars",ylim=c(0,400))
```

## Scatter plots and correlation
In the biomedical sciences it is common to be interested in the relationship between two or more variables. A classic examples is the father/son height data used by Galton to understand the heredity. Were we to summarize these data we could use the two average and two standard deviation as both are well approximated by the normal distribution. This summary however fails to describe an important characteristic of the data.

```{r,fig.width=4, fig.height=4}
data("father.son")
x=father.son$fheight
y=father.son$sheight
plot(x,y,xlab="Father's height in inches",ylab="Son's height in inches",main=paste("correlation =",signif(cor(x,y),2)))
```
The scatter plot shows a general trend: the taller the father the taller to son. A summary of this trend is the correlation coefficient which in this cases is 0.5. We motivate this statistic by trying to predict son's height using the father's.

## Stratification
Suppose we are asked to guess the height of randomly select height from the sons. The average height, 68.7 inches, is the value with the highest proportion (see histogram) and would be our prediction. But what if we are told that the father is 72 inches tall, do we sill guess 68.7?

Note that the father is taller than average. He is 1.7 standard deviations taller than the average father. So should we predict that the son is also 1.75 standard deviations taller? Turns out this is an overestimate. To see this we look at all the sons with fathers who are about 72 inches. We do this by _stratifying_ the son heights.
```{r,fig.width=6, fig.height=3}
boxplot(split(y,round(x)))
print(mean(y[ round(x) == 72]))
```
Stratification followed by boxplots lets us see the distribution of each group. The average height of sons with fathers that are 72 is 70.7. We also see that the means of the strata follow a straight line. This line is refereed to the regression line and it's slope related to the correlation. When two variables follow a bivariate normal distribution then for any given value of x we predict the value of y with
$$
\frac{Y - \mu_Y}{\sigma_Y} = r \frac{X-\mu_X}{\sigma_X}
$$
with the $\mu$ representing the averages, $\sigma$ the standard deviations, and $r$ the correlation. Let's compare the mean of each strata to the identity line and the regression line

```{r,fig.width=3, fig.height=3}
means=tapply(y,round(x),mean)
fatherheights=as.numeric(names(means))
plot(fatherheights,means,ylab="average of strata of son heights",ylim=range(fatherheights))
abline(0,1)
abline(lm(y~x),lty=2)
```

## Spearman's correlation
Just like the average and standard deviation are not good summaries when the data is not well approximated by the normal distribution, the correlation is not a good summary when pairs of lists are not approximated by the bivariate normal distribution. Examples include cases in which on variable is related to another by a parabolic function. Another, more common example are caused by outliers or extreme values.

```{r,fig.width=3, fig.height=3}
a=rnorm(100);a[1]=10
b=rnorm(100);b[1]=11
plot(a,b,main=paste("correlation =",signif(cor(a,b),2)))
```
In the example above the data are not associated but for one pair both values are very large. The correlation here is about 0.5. This is driven by just that one point as taking it out lowers to correlation to about 0. An alternative summary for cases with outliers or extreme values is Spearman's correlation which is based on ranks instead of the values themselves. 

## Microarray data
Here we are analyzing microarray data from eight samples: two groups of four. A first step in any analysis of genomics data is to learn its general properties and search for problematic samples. By viewing the data from the first sample we immediately notice that over 90% of data is below 1,000 and the remaining 10% spans values up to 40,000. By taking the log we get a better picture of the distribution. We use base 2 because memorizing the powers of 2 is easy. It gives us a friendly range: 4-16. 

```{r,fig.width=6, fig.height=3}
library(rafalib)
# this needs to be be downloaded with
# install("devtools")
# library(devtools)
# install_github("dagEDA","mikelove")
library(dagEDA)
data(SpikeInEDA)
mypar(1,2)
hist(int[,1])
hist(log2(int[,1]))
```

Next we look at all eight histograms simultaneously. To facilitate this we introduce the _density estimator_ or _smooth histogram_. Basically we create a histogram, draw a smooth curve through the top of the bars, and keep that curve. This permits us to put several histograms on the same page:

```{r,fig.width=3, fig.height=3}
for(i in 1:ncol(int))
  shist(log2(int[,i]),unit=0.25,add=!i==1,plotHist=FALSE,lineCol=(i==4)+1)
```
Note that one histogram (we higlighted it by making it red) looks different: it has a different shape from the rest. So is this sample different from the rest in any significant way? If we compute the correlation between this sample and the rest it is not very different and all very high.
```{r}
signif(cor(int),2)
```
The problem is not immediately obviou from a scatter plot.
```{r,fig.width=6, fig.height=3}
##we don't need to show all the points so we take samples
splot<-function(x,y,...){
  ind<-sample(length(x),10000)
  x=x[ind];y=y[ind] 
  plot(x,y,...)
}  
mypar(1,2)
splot(log2(int[,1]),log2(int[,2])) 
splot(log2(int[,1]),log2(int[,4]))
```
Note that samples 1 through 4 are replicates and should produce the same values up to measurement error. Scatterplots and correlation are not the best tools to detect problems. Note for example that  1,2,3,4 and 100,200,300,400 two lists with very different values have perfect correlation. A better measure is the differences between the values and therefore a better plot is a rotation of the scatter plot containg the differences (log ratios) on the y-axis and the averages (in the log scale) on the x-axis. This plot is a refered to as an MA-plot. 
```{r,fig.width=6, fig.height=3}
maplot<- function(x,y,...) splot((x+y)/2,y-x,...)
mypar(1,3)
maplot(log2(int[,1]),log2(int[,2]),xlab="A",ylab="M",ylim=c(-2,2))
maplot(log2(int[,1]),log2(int[,3]),xlab="A",ylab="M",ylim=c(-2,2))
maplot(log2(int[,1]),log2(int[,4]),xlab="A",ylab="M",ylim=c(-2,2))
```
Now the problem is obvious. It turns out this samples comes from an array for which a spatial problem can be detected at the original image level. We actually have the grid locations for these measurements and can recreate the image.

```{r, fig.width=6, fig.height=3}
##we are doing this for two arrays 1 and 4
library(matrixStats) ##need rowMedians
for(i in c(1,4)){
  r=log2(int[,i])-rowMedians(log2(int)) 
  ## r are residuals from median array
  ## to avoind outliers taking over colors of image
  ### define a MAX
  MAX<-1
  r[r>MAX]<-MAX
  r[r< -MAX] <- -MAX
  ##we now that every other column is skipped
  mat=matrix(NA,max(locations[,1]),max(locations[,2]+1)/2)
  for(j in 1:nrow(locations)){
     mat[locations[j,1],(locations[j,2]+1)/2]<-r[j]
  }
  image(mat,col=brewer.pal(11,"RdBu"))
}
```

On the second image we can clearly see the spatial pattern (blue are positive residuals, red are negative)

## NGS

This is a dataset produced by Bottomly et al., sequencing two strains of mouse with many biological replicates. This dataset and a number of other sequencing datasets have been compiled from raw data into read counts tables by Frazee, Langmead, and Leek as part of the ReCount project. These datasets are made publicly available at the following website:

http://bowtie-bio.sourceforge.net/recount/

Unlike many sequencing studies, Bottomly et al., realizing the such information is important for downstream analysis, provided the experiment number for all samples. Below we can see that the experimental batch explains more variation than the condition of interest: the strain of mouse. 

We can make similar figures for NGS to the ones shown in the previous sections. However, the log transform does not work because RNAseq data contains many 0s. One quick way to get around this is by adding a constant before taking the log. A typical one is 0.5 which gives us a log2 value of -1 for 0s.

```{r}
download.file("http://bowtie-bio.sourceforge.net/recount/ExpressionSets/bottomly_eset.RData","bottomly_eset.RData")
load("bottomly_eset.RData")
library("Biobase")
Y <- log2(exprs(bottomly.eset) + 0.5)
mypar(1,1)
for(i in 1:ncol(Y)){
  shist(Y[,i],unit=0.25,lineCol=i,plotHist=FALSE,add=i!=1)
}
```

