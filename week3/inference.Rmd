# Statistical Inference

In biology journals we see many figures such as *Figure*-page 3. Today we will learn what the litte star means. In most genome wide association studies we see Manhattan plots (*Figure* page 4). We will leanr what _P_ means. 

In science it is common to ask if two things are different
Are men taller than women? Is the risk of cancer different in smokers and non-smokers? Is the probability of getting type II different for different genetic backgrounds? Is this gene differentially expressed in cancer? When we make two measurements and compare, we almost always see some difference. But will wee see it again if we measure again? If someone else measures? Statistical inference can help us answer this question.

## Association tests

One of the most famous examples of hypothesis testing was performed by RA Fisher on a lady that claimed could tell if milk was added before or after the tea was poured. Fisher gave the lady four pairs of cups of tea: one with milk poured first, the other after. The order was randomized. Say the lady picked 3 out 4 correctly, do we believe she has a special ability? Hypothesis testing helps answer this question by quantifying what happens by chance.

The basic question we ask is: if the lady is just guessing, what are the chances that she gets 3 or more correct? When we use a statistic and it’s probability distribution to answer this question we call it a _hypothesis test_ or test for short.

If we assume the lady is just guessing randomly, we can think of this particular examples as picking 4 balls out of an urn with 4 green (correct answer) and 4 red (incorrect answer) balls. 

Under the _null hypothesis_ that the lady is just guessing each ball has the same chance of being picked. We can then use combinatorics to figure out the probability. The probability of picking 3 is 
${4 \choose 3} {4 \choose 1} / {8 \choose 4} = 16/70$.  
The probability of picking all correct is
${4 \choose 4} {4 \choose 0}/{8 \choose 4}= 1/70$. Thus the chance of observing a 3 or something more extreme, under the null hypothesis, is 0.24. This is called a p-value. This is called Fisher's exact test and it uses the hyper geometric distribution. It is not appropirate for most the tests applied in genetics but the idea is similar.

The y-axis of a Manhattan plot (*Figure* - page 4) is typically represents the netative of log (base 10) of the p-values obtained for association tests applied at each SNP. 

For example, imagine we have 280 individuals, some of them have a given disease others don’t. We observe that a 20% of the individuals that are homozygous for the minor allele have the disease compared to 10% of the rest. Would we see this again if we picked another 220 individuals?

Here is an example dataset
```{r}
disease=c(rep(0,180),rep(1,20),rep(0,40),rep(1,10))
genotype=c(rep("AA",200),rep("aa",50))
tab=table(genotype,disease)
tab
```

The null-hypothesis is that the 200 and 50 individuals in each group were assigned disease with the same probability. If this is the case then the probability of disease is
```{r}
p=mean(disease)
p
```
The expected table is therefore
```{r}
rbind(c(1-p,p)*sum(genotype=="aa"),c(1-p,p)*sum(genotype=="AA"))
```

Using an asymptotic result about the sums of independent binary outcomes, we can compute an approximate probability of seeing a deviation for the expected table as big as this one.
The p-value for this table is 
```{r}
chisq.test(tab)$p.value
```
Note that there is not a one to one relationship between the odds ratio and the p-value. If increase the numbers but keep the difference in proportions the same, the p-value is reduced substantially:
```{r}
tab=tab*10
chisq.test(tab)$p.value
```

## The t-test
Suppose we have never seen men and women before and are asked if the in a given population of 25,000 people, men are, on average, taller than women. Suppose we are charged $1000 for each measurement and are given a $50,000 prize if we answer correclty, how many men and women do we measure? Let's start with 10

```{r}
datadir="http://www.biostat.jhsph.edu/bstcourse/bio751/data"
dat=read.csv(file.path(datadir,"USheights_subsample.csv"))
set.seed(1)
men=sample(dat$Height[dat$Gender==1],10)
women=sample(dat$Height[dat$Gender==0],10)
```
Here is a quick plot
```{r}
stripchart(list(women,men),vertical=TRUE)
```
We want to know if the difference between the average heights are positive, negative, or practically 0.  The observed difference is a mere 3.3 inches. 
```{r}
mean(men)-mean(women)
```
Will this difference hold up if we take another sample? Remeber we have to pay for each measurement. 

### Central limit theorem

The Central Limit Theorem (or CLT) is one of the most used mathematical results in science. It tells us that when the sample size is large the average of random a sample follows a normal distribution centered at the population average (what we want to know), call it $\mu$, and with standard deviation equal to the population standard deviation, call it $\sigma$, divded by the square root of the sample size. This implies that under the null hypothese that there is no difference between the population averages, the difference between the sample averages $\hat{Y}-\hat{X}$ with $\hat{X}$ and $\hat{Y}$ the sample average for women and men respectiveley, is approximated by a normal distribution centered at 0 (there is no difference) and with standard deviation $\sqrt{\sigma_X^2 +\sigma_Y^2}/\sqrt{N}$. To see that this is the standard devition you need to know that the varaince of the sum of two independent random varialbes is the sum of their variances and the standard deviation is the square root of the variance. 

This is imply that this ratio:
$$
\sqrt{N}\frac{\bar{Y}-\bar{X}}{\sqrt{\sigma_X^2 +\sigma_Y^2}}
$$
is approximated by a normal distribution centered at 0 and standard deviation 1.  Using this approximation make computing p-values simple because we know the proportion of the distribtuion under value. For example, only 5% values of larger than 2 (in absolute value):
```{r}
1-pnorm(2)+pnorm(-2)
```
So what is the p-value of our observed differences? Note that we can't compute the ratio above because we don't know the population standard deviations: $\sigma_X$ and $\sigma_Y$. If we use the sample standard deviations, call then $\hat{\sigma}_X$ and $\hat{\sigma}_Y$ we form what is refered to the t-test, a quantity we can actually compute:
$$
\sqrt{N} \frac{\bar{Y}-\bar{X}}{\sqrt{\hat{\sigma}_X^2 +\hat{\sigma}_Y^2}}
$$
It turns out that for large enough N, the t-statistic is approximated by a normal distribution centered at 0 and with standard deviation 1. Our t-test is certainly unlikely to occur:

```{r}
ttest<-sqrt(length(men)*(mean(men)-mean(women))/sqrt((var(men)+var(women))))
ttest
2*pnorm(-ttest)
```
But what is large enough? 30 is a rule a thumb and it implies our 10 is not enough. But for cases were the population values are normally distributed (like heights) the exact distribution can be derived and it is the t-distribution (where the t-statistic gets it's name). Not surprisingly the t-distribution has fatter tails (a bigged proportion of larger values) than the normal distribution since the standard deviation estimates add variability (the numerator is normal in this case). We can now obtain a p-value (we need to specify the _degrees of freedom_, which in the case of the sample ttest is the sum of the sample sizes minus 2.
```{r}
1-pt(ttest,20-2)+pt(-ttest,20-2)##or
2*pt(-ttest,20-2)
```

As expected the p-value using the t-distribution is larger than the one obtained with the normal approximation which gives underestimates when N is small. Regardless, it seems the chance of observing a difference as large as what we saw under the null is pretty small. We should correctly predict that the average height of men is larger than the average height of women and also confirm that the distributions are approximately normal 

```{r}
mean(dat$Height[dat$Gender==1])-mean(dat$Height[dat$Gender==0])
library(rafalib)
shist(dat$Height[dat$Gender==1],plotHist=FALSE,lineCol=1,unit=1,xlim=range(dat$Height))
shist(dat$Height[dat$Gender==0],plotHist=FALSE,lineCol=2,add=TRUE,unit=1)
```

Homework: is the distribution of dat$Gener approximated by normal, t or neither?

Note that we obtained an estimate but never really reported it. We simply reported the p-value. Although common practice, we do not recommend it. Note that we can obtain statistically significant results that are not scientifically significant. The prefered way to report the estimate is the present both the estimate and its standard deviation: 
```{r}
cat(mean(men)-mean(women),"+/-",sqrt(var(men)+var(women))/sqrt(length(men)),"\n")
```
We can also _confidence intervals_ which should fall on the true difference for 95% of the random samples one could take.
```{r}
cat(mean(men)-mean(women)+c(-2,2)*sqrt(var(men)+var(women))/sqrt(length(men)),sep=",")
```
Note that the true difference is in fact included in the sample above

##Gene expression


















So what if we don't have enough data? 
















